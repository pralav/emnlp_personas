<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Overview | Learning personas</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Overview" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Memory-Augmented Video Semantic Network" />
<meta property="og:description" content="Memory-Augmented Video Semantic Network" />
<link rel="canonical" href="https://pralav.github.io/Video-SemNet/" />
<meta property="og:url" content="https://pralav.github.io/Video-SemNet/" />
<meta property="og:site_name" content="Video-SemNet" />
<script type="application/ld+json">
{"name":"Video-SemNet","description":"Memory-Augmented Video Semantic Network","@type":"WebSite","url":"https://pralav.github.io/Video-SemNet/","headline":"Overview","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Video-SemNet/assets/css/style.css?v=0f45bab284ffadb553c5e68e45abfd2564884bf0">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://pralav.github.io/Video-SemNet/">Video-SemNet</a></h1>
        
        

        <p>Memory-Augmented Video Semantic Network</p>

        

        
      </header>
      <section>

      <h2 id="overview">Overview</h2>

<p>Memory-Augmented Video Semantic Network for learning to:
  <ul>
    <li>Encode semantic descriptors of video into an embedding</li>
    <li>Predict genres and IMDB ratings</li>
  </ul>
</p>

<p><img src="https://github.com/pralav/video-semnet/raw/master/samsung.001.jpeg" alt="Image" /></p>

<h2 id="paper">Paper</h2>
<p><a href="https://nips2017vigil.github.io/">Conference of Neural Information Processing Systems (NIPS 2017): Visually-Grounded Interaction and Language Workshop</a> : <a href="https://github.com/nips2017vigil/nips2017vigil.github.io/raw/master/papers/2017/video-semnet-memory.pdf">PDF</a></p>

<h2 id="datasets">Datasets</h2>
      <h4 id="movie-clip-dataset">Movie clip dataset</h4>
The dataset attached here is a subset of samples that were used for training our model. We use several such video segments from the same movie to train our model.
    <ul>
  <li><a href="https://s3.amazonaws.com/storytelling-data/movie_clips_full.csv">Full annotations (.csv)</a></li>
  <li><a href="https://s3.amazonaws.com/storytelling-data/movie_clips_aggregated.csv">Aggregated annotations (.csv)</a></li>
</ul>



<h2 id="contact">Contact</h2>

<p>Have a question or comment? Please contact us at  pralav [at] mit [dot] edu</p>


      </section>
      <footer>
        
        <p>This research project is part of the  
         <a href="https://www.media.mit.edu/groups/social-machines/overview/">Laboratory for Social Machines</a> at the MIT Media Lab</p>
        
      </footer>
    </div>
    <script src="/Video-SemNet/assets/js/scale.fix.js"></script>
    
  </body>
</html>
